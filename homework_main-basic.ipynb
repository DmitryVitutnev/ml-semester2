{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Basic Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design was heavily inspired by [PyTorch](http://pytorch.org) which is the main framework of our course "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework requires sending **multiple** files, please do not forget to include all the files when sending to TA. The list of files:\n",
    "- This notebook\n",
    "- homework_modules.ipynb with all blocks implemented (except maybe `Conv2d` and `MaxPool2d` layers implementation which are part of 'advanced' version of this homework)\n",
    "- homework_differentiation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K     |████████████████████████████████| 753.4MB 17kB/s  eta 0:00:01    |▉                               | 18.8MB 4.3MB/s eta 0:02:51     |█▍                              | 33.3MB 1.3MB/s eta 0:09:09     |█▌                              | 35.7MB 1.5MB/s eta 0:08:03     |█▋                              | 38.2MB 2.9MB/s eta 0:04:06     |████▋                           | 107.6MB 388kB/s eta 0:27:41     |████▋                           | 108.2MB 916kB/s eta 0:11:45     |████▊                           | 109.9MB 878kB/s eta 0:12:13     |█████                           | 115.8MB 441kB/s eta 0:24:04     |█████                           | 118.9MB 1.6MB/s eta 0:06:33     |██████▎                         | 147.9MB 1.1MB/s eta 0:09:32     |████████                        | 188.0MB 2.0MB/s eta 0:04:42     |█████████▋                      | 225.3MB 2.0MB/s eta 0:04:20     |██████████                      | 234.0MB 989kB/s eta 0:08:46     |██████████                      | 234.2MB 989kB/s eta 0:08:45     |██████████▍                     | 244.9MB 1.9MB/s eta 0:04:30     |███████████                     | 260.3MB 1.6MB/s eta 0:05:08     |████████████▍                   | 292.5MB 910kB/s eta 0:08:27     |██████████████▌                 | 342.7MB 1.9MB/s eta 0:03:31     |██████████████▊                 | 345.8MB 2.0MB/s eta 0:03:29     |███████████████▋                | 368.4MB 1.1MB/s eta 0:06:04     |███████████████▋                | 368.7MB 1.1MB/s eta 0:06:04     |████████████████▎               | 382.7MB 2.7MB/s eta 0:02:20     |█████████████████               | 398.3MB 302kB/s eta 0:19:36     |█████████████████               | 398.6MB 50kB/s eta 1:57:39     |█████████████████               | 399.8MB 505kB/s eta 0:11:40     |█████████████████▍              | 409.0MB 2.5MB/s eta 0:02:20     |█████████████████▍              | 409.6MB 2.5MB/s eta 0:02:20     |█████████████████▋              | 414.7MB 617kB/s eta 0:09:09     |██████████████████▍             | 432.3MB 1.9MB/s eta 0:02:51     |███████████████████▎            | 454.8MB 2.2MB/s eta 0:02:17     |███████████████████▌            | 458.2MB 809kB/s eta 0:06:05     |███████████████████▋            | 461.3MB 250kB/s eta 0:19:28     |████████████████████▏           | 473.9MB 649kB/s eta 0:07:11     |████████████████████▏           | 475.2MB 656kB/s eta 0:07:04     |████████████████████▍           | 480.8MB 730kB/s eta 0:06:14     |█████████████████████▏          | 498.1MB 974kB/s eta 0:04:22     |█████████████████████▋          | 509.1MB 1.5MB/s eta 0:02:46     |█████████████████████▉          | 514.0MB 1.1MB/s eta 0:03:43     |██████████████████████▎         | 523.4MB 994kB/s eta 0:03:52     |██████████████████████▋         | 532.3MB 3.3MB/s eta 0:01:07��█████████████████████▊         | 534.6MB 1.1MB/s eta 0:03:13��█████████████████████▉         | 536.5MB 3.9MB/s eta 0:00:57     |███████████████████████         | 541.2MB 729kB/s eta 0:04:52     |███████████████████████▎        | 549.2MB 1.0MB/s eta 0:03:25     |███████████████████████▍        | 551.9MB 1.6MB/s eta 0:02:038MB 1.4MB/s eta 0:02:21     |████████████████████████        | 563.0MB 846kB/s eta 0:03:45     |████████████████████████▍       | 575.4MB 1.8MB/s eta 0:01:37     |████████████████████████▊       | 581.1MB 349kB/s eta 0:08:14     |█████████████████████████       | 588.9MB 1.3MB/s eta 0:02:08     |█████████████████████████▏      | 593.2MB 597kB/s eta 0:04:29     |█████████████████████████▎      | 594.6MB 597kB/s eta 0:04:26     |█████████████████████████▍      | 597.9MB 5.0MB/s eta 0:00:32��█████████████████▌      | 599.4MB 5.0MB/s eta 0:00:32     |█████████████████████████▌      | 600.2MB 5.0MB/s eta 0:00:31     |█████████████████████████▋      | 604.3MB 628kB/s eta 0:03:58     |██████████████████████████▊     | 628.5MB 923kB/s eta 0:02:16     |██████████████████████████▉     | 633.0MB 1.1MB/s eta 0:01:55     |███████████████████████████▊    | 651.5MB 1.2MB/s eta 0:01:28     |███████████████████████████▊    | 653.5MB 1.1MB/s eta 0:01:29��█████████▏   | 662.0MB 6.0MB/s eta 0:00:16     |████████████████████████████▋   | 673.3MB 655kB/s eta 0:02:03     |████████████████████████████▉   | 678.2MB 908kB/s eta 0:01:23     |█████████████████████████████   | 684.1MB 997kB/s eta 0:01:10     |█████████████████████████████   | 685.1MB 1.0MB/s eta 0:01:08     |█████████████████████████████▏  | 685.8MB 1.0MB/s eta 0:01:07     |█████████████████████████████▎  | 690.1MB 2.6MB/s eta 0:00:24��████████████████████▌  | 693.7MB 2.9MB/s eta 0:00:21     |█████████████████████████████▊  | 701.1MB 363kB/s eta 0:02:25     |█████████████████████████████▉  | 702.6MB 264kB/s eta 0:03:12     |██████████████████████████████▏ | 710.7MB 1.0MB/s eta 0:00:43    |██████████████████████████████▎ | 714.1MB 2.0MB/s eta 0:00:20     |██████████████████████████████▊ | 722.6MB 923kB/s eta 0:00:34     |██████████████████████████████▊ | 722.7MB 1.1MB/s eta 0:00:293.2MB 3.2MB/s eta 0:00:073.5MB 3.2MB/s eta 0:00:07██████▏| 734.0MB 3.2MB/s eta 0:00:075.2MB 2.3MB/s eta 0:00:088.1MB 2.6MB/s eta 0:00:069.9MB 2.6MB/s eta 0:00:06     |███████████████████████████████▋| 743.6MB 344kB/s eta 0:00:29\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement everything in `Modules.ipynb`. Read all the comments thoughtfully to ease the pain. Please try not to change the prototypes.\n",
    "\n",
    "Do not forget, that each module should return **AND** store `output` and `gradInput`.\n",
    "\n",
    "The typical assumption is that `module.backward` is always executed after `module.forward`,\n",
    "so `output` is stored, this would be useful for `SoftMax`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech note\n",
    "Prefer using `np.multiply`, `np.add`, `np.divide`, `np.subtract` instead of `*`,`+`,`/`,`-` for better memory handling.\n",
    "\n",
    "Example: suppose you allocated a variable \n",
    "\n",
    "```\n",
    "a = np.zeros(...)\n",
    "```\n",
    "So, instead of\n",
    "```\n",
    "a = b + c  # will be reallocated, GC needed to free\n",
    "``` \n",
    "You can use: \n",
    "```\n",
    "np.add(b,c,out = a) # puts result in `a`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/programms/semester2/homework_modules.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FOR TESTS ONLY!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# (re-)load layers\n",
    "%run homework_modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "N = 500\n",
    "\n",
    "X1 = np.random.randn(N,2) + np.array([2,2])\n",
    "X2 = np.random.randn(N,2) + np.array([-2,-2])\n",
    "\n",
    "Y = np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
    "Y = np.hstack([Y, 1-Y])\n",
    "\n",
    "X = np.vstack([X1,X2])\n",
    "plt.scatter(X[:,0],X[:,1], c = Y[:,0], edgecolors= 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **logistic regression** for debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 2))\n",
    "net.add(LogSoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(net)\n",
    "\n",
    "# Test something like that then \n",
    "\n",
    "# net = Sequential()\n",
    "# net.add(Linear(2, 4))\n",
    "# net.add(ReLU())\n",
    "# net.add(Linear(4, 2))\n",
    "# net.add(LogSoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with batch_size = 1000 to make sure every step lowers the loss, then try stochastic version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iptimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(dataset, batch_size):\n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic training loop. Examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "        \n",
    "        net.zeroGradParameters()\n",
    "        \n",
    "        # Forward\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "    \n",
    "        # Backward\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        \n",
    "        # Update weights\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "        \n",
    "        loss_history.append(loss)\n",
    "\n",
    "    # Visualize\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "        \n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Current loss: %f' % loss)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using old good [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = mnist.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compare** `ReLU`, `ELU`, `LeakyReLU`, `SoftPlus` activation functions. \n",
    "You would better pick the best optimizer params for each of them, but it is overkill for now. Use an architecture of your choice for the comparison.\n",
    "- **Try** inserting `BatchNormalization` (folowed by `ChannelwiseScaling`) between `Linear` module and activation functions.\n",
    "- Plot the losses both from activation functions comparison and `BatchNormalization` comparison on one plot. Please find a scale (log?) when the lines are distinguishable, do not forget about naming the axes, the plot should be goodlooking.\n",
    "- Plot the losses for two networks: one trained by momentum_sgd, another one trained by Adam. Which one performs better?\n",
    "- Hint: good logloss for MNIST should be around 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your personal opinion on the activation functions, think about computation times too. Does `BatchNormalization` help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, use all your knowledge to build a super cool model on this dataset. Use **dropout** to prevent overfitting, play with **learning rate decay**. You can use **data augmentation** such as rotations, translations to boost your score. Use your knowledge and imagination to train a model. Don't forget to call `training()` and `evaluate()` methods to set desired behaviour of `BatchNormalization` and `Dropout` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print here your accuracy on test set. It should be around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
